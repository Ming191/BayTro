"""
Evaluation Router for Chatbot Quality Assessment
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from services.evaluation_service import EvaluationService
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

# Initialize evaluation service only
try:
    evaluation_service = EvaluationService()
    logger.info("Evaluation service initialized successfully")
except Exception as e:
    logger.error(f"Failed to initialize Evaluation service: {e}")
    evaluation_service = None

# Import graphrag service from chatbot router to avoid duplicate initialization
graphrag_service = None

def get_graphrag_service():
    """Get GraphRAG service from chatbot router"""
    global graphrag_service
    if graphrag_service is None:
        try:
            from routers.chatbot import graphrag_service as chatbot_graphrag
            graphrag_service = chatbot_graphrag
            logger.info("Using GraphRAG service from chatbot router")
        except Exception as e:
            logger.error(f"Failed to get GraphRAG service: {e}")
    return graphrag_service


# Request/Response Models
class RetrievalEvalRequest(BaseModel):
    retrieved_node_ids: List[str] = Field(..., description="List of retrieved node IDs")
    ground_truth_node_ids: List[str] = Field(..., description="List of ground truth relevant node IDs")


class GenerationEvalRequest(BaseModel):
    generated_answer: str = Field(..., description="Generated answer from chatbot")
    reference_answer: str = Field(..., description="Reference/ground truth answer")


class LLMJudgeEvalRequest(BaseModel):
    question: str = Field(..., description="User's question")
    answer: str = Field(..., description="Generated answer")
    context: List[str] = Field(..., description="Context texts used for generation")
    metric: str = Field(..., description="Metric to evaluate: faithfulness, answer_relevance, or overclaiming")


class CompleteEvalRequest(BaseModel):
    question: str = Field(..., description="User's question")
    generated_answer: Optional[str] = Field(None, description="Generated answer (if None, will be generated)")
    retrieved_node_ids: Optional[List[str]] = Field(None, description="Retrieved node IDs")
    retrieved_contexts: Optional[List[str]] = Field(None, description="Retrieved contexts")
    ground_truth_node_ids: Optional[List[str]] = Field(None, description="Ground truth node IDs for retrieval eval")
    reference_answer: Optional[str] = Field(None, description="Reference answer for generation eval")
    user_role: Optional[str] = Field("tenant", description="User role for question answering")
    auto_generate: bool = Field(True, description="Auto-generate answer if not provided")
    question_type: Optional[str] = Field(None, description="Type of question: irrelevant, direct, paraphrased, multiple_points")


class BatchEvalRequest(BaseModel):
    test_cases: List[Dict[str, Any]] = Field(..., description="List of test cases with questions and ground truths")
    user_role: str = Field("tenant", description="User role for evaluation")


# Endpoints
@router.get("/health")
async def health_check():
    """
    Health check endpoint for evaluation service
    """
    graphrag = get_graphrag_service()
    return {
        "status": "ok",
        "evaluation_service": "available" if evaluation_service is not None else "unavailable",
        "graphrag_service": "available" if graphrag is not None else "unavailable"
    }


@router.post("/retrieval")
async def evaluate_retrieval(request: RetrievalEvalRequest):
    """
    Evaluate retrieval quality using Precision, Recall, F1-score, and Hit Rate

    - **retrieved_node_ids**: IDs of nodes retrieved by the system
    - **ground_truth_node_ids**: IDs of nodes that are actually relevant
    """
    if evaluation_service is None:
        raise HTTPException(status_code=503, detail="Evaluation service not available")

    try:
        results = evaluation_service.evaluate_retrieval(
            retrieved_node_ids=request.retrieved_node_ids,
            ground_truth_node_ids=request.ground_truth_node_ids
        )
        return results
    except Exception as e:
        logger.error(f"Error in retrieval evaluation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generation/cosine-similarity")
async def evaluate_generation(request: GenerationEvalRequest):
    """
    Evaluate generation quality using cosine similarity with OpenAI embeddings

    - **generated_answer**: The answer generated by the chatbot
    - **reference_answer**: The reference/ground truth answer
    """
    if evaluation_service is None:
        raise HTTPException(status_code=503, detail="Evaluation service not available")

    try:
        results = await evaluation_service.evaluate_generation_cosine_similarity(
            generated_answer=request.generated_answer,
            reference_answer=request.reference_answer
        )
        return results
    except Exception as e:
        logger.error(f"Error in generation evaluation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/llm-judge")
async def evaluate_with_llm_judge(request: LLMJudgeEvalRequest):
    """
    Evaluate using LLM as a judge

    - **metric**: 'faithfulness', 'answer_relevance', or 'overclaiming'
    - **question**: User's question
    - **answer**: Generated answer
    - **context**: Context texts (required for faithfulness and overclaiming)
    """
    if evaluation_service is None:
        raise HTTPException(status_code=503, detail="Evaluation service not available")

    try:
        if request.metric == "faithfulness":
            results = await evaluation_service.evaluate_faithfulness(
                question=request.question,
                answer=request.answer,
                context=request.context
            )
        elif request.metric == "answer_relevance":
            results = await evaluation_service.evaluate_answer_relevance(
                question=request.question,
                answer=request.answer
            )
        elif request.metric == "overclaiming":
            results = await evaluation_service.evaluate_overclaiming(
                answer=request.answer,
                context=request.context
            )
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Unknown metric: {request.metric}. Use 'faithfulness', 'answer_relevance', or 'overclaiming'"
            )

        return results
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in LLM judge evaluation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/complete")
async def evaluate_complete(request: CompleteEvalRequest):
    """
    Run complete evaluation suite on a single question

    Automatically generates answer if not provided and evaluates with all available metrics.
    """
    if evaluation_service is None:
        raise HTTPException(status_code=503, detail="Evaluation service not available")

    try:
        generated_answer = request.generated_answer
        retrieved_node_ids = request.retrieved_node_ids or []
        retrieved_contexts = request.retrieved_contexts or []

        # Auto-generate answer if needed
        if not generated_answer and request.auto_generate:
            service = get_graphrag_service()
            if service is None:
                raise HTTPException(status_code=503, detail="GraphRAG service not available for auto-generation")

            logger.info(f"Auto-generating answer for question: {request.question[:100]}...")
            result = await service.query(
                question=request.question,
                conversation_history=None,
                user_role=request.user_role
            )

            generated_answer = result['answer']
            retrieved_node_ids = [node['id'] for node in result.get('context', [])]
            retrieved_contexts = [node['content'] for node in result.get('context', [])]

            logger.info(f"Generated answer with {len(retrieved_node_ids)} retrieved nodes")
            logger.info(f"Retrieved node IDs: {retrieved_node_ids}")
            logger.info(f"Ground truth node IDs: {request.ground_truth_node_ids}")

        # Log comparison for debugging hit rate issues
        if request.ground_truth_node_ids and retrieved_node_ids:
            logger.info("="*60)
            logger.info("RETRIEVAL DEBUG INFO:")
            logger.info(f"Expected (Ground Truth): {request.ground_truth_node_ids}")
            logger.info(f"Retrieved: {retrieved_node_ids}")
            logger.info(f"Match found: {any(gt in retrieved_node_ids for gt in request.ground_truth_node_ids)}")
            logger.info("="*60)

        # Run complete evaluation
        results = await evaluation_service.evaluate_complete(
            question=request.question,
            generated_answer=generated_answer,
            retrieved_node_ids=retrieved_node_ids,
            retrieved_contexts=retrieved_contexts,
            ground_truth_node_ids=request.ground_truth_node_ids,
            reference_answer=request.reference_answer,
            question_type=request.question_type
        )

        # Add retrieved info to response for debugging
        results["retrieved_node_ids"] = retrieved_node_ids
        results["retrieved_contexts"] = retrieved_contexts

        return results
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in complete evaluation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/batch")
async def evaluate_batch(request: BatchEvalRequest):
    """
    Run evaluation on multiple test cases

    Each test case should have:
    - question: str (required)
    - ground_truth_node_ids: List[str] (optional)
    - reference_answer: str (optional)
    """
    if evaluation_service is None:
        raise HTTPException(status_code=503, detail="Evaluation service not available")

    service = get_graphrag_service()
    if service is None:
        raise HTTPException(status_code=503, detail="GraphRAG service not available for batch evaluation")

    try:
        results = []
        total_cases = len(request.test_cases)

        for idx, test_case in enumerate(request.test_cases):
            logger.info(f"Evaluating test case {idx + 1}/{total_cases}")

            question = test_case.get("question")
            if not question:
                logger.warning(f"Test case {idx + 1} missing question, skipping")
                continue

            # Generate answer
            query_result = await service.query(
                question=question,
                conversation_history=None,
                user_role=request.user_role
            )

            generated_answer = query_result['answer']
            retrieved_node_ids = [node['id'] for node in query_result.get('context', [])]
            retrieved_contexts = [node['content'] for node in query_result.get('context', [])]

            # Evaluate
            eval_result = await evaluation_service.evaluate_complete(
                question=question,
                generated_answer=generated_answer,
                retrieved_node_ids=retrieved_node_ids,
                retrieved_contexts=retrieved_contexts,
                ground_truth_node_ids=test_case.get("ground_truth_node_ids"),
                reference_answer=test_case.get("reference_answer")
            )

            results.append({
                "test_case_id": idx,
                "question": question,
                "evaluation": eval_result
            })

        # Calculate aggregate statistics
        aggregate = _calculate_aggregate_stats(results)

        return {
            "total_cases": total_cases,
            "evaluated_cases": len(results),
            "results": results,
            "aggregate_statistics": aggregate
        }

    except Exception as e:
        logger.error(f"Error in batch evaluation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _calculate_aggregate_stats(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Calculate aggregate statistics from batch evaluation results"""
    if not results:
        return {}

    # Collect all metrics
    retrieval_metrics = []
    generation_metrics = []
    faithfulness_scores = []
    relevance_scores = []
    overclaiming_scores = []

    for result in results:
        eval_data = result.get("evaluation", {})
        summary = eval_data.get("summary", {})

        if "retrieval_score" in summary:
            retrieval_metrics.append(summary["retrieval_score"])

        if "generation_score" in summary:
            generation_metrics.append(summary["generation_score"])

        if "faithfulness_score" in summary:
            faithfulness_scores.append(summary["faithfulness_score"])

        if "relevance_score" in summary:
            relevance_scores.append(summary["relevance_score"])

        if "overclaiming_score" in summary:
            overclaiming_scores.append(summary["overclaiming_score"])

    # Calculate averages
    aggregate = {}

    if retrieval_metrics:
        aggregate["avg_retrieval_score"] = round(sum(retrieval_metrics) / len(retrieval_metrics), 4)

    if generation_metrics:
        aggregate["avg_generation_score"] = round(sum(generation_metrics) / len(generation_metrics), 4)

    if faithfulness_scores:
        aggregate["avg_faithfulness"] = round(sum(faithfulness_scores) / len(faithfulness_scores), 4)

    if relevance_scores:
        aggregate["avg_relevance"] = round(sum(relevance_scores) / len(relevance_scores), 4)

    if overclaiming_scores:
        aggregate["avg_overclaiming"] = round(sum(overclaiming_scores) / len(overclaiming_scores), 4)

    # Overall score
    all_scores = []
    if retrieval_metrics:
        all_scores.extend(retrieval_metrics)
    if generation_metrics:
        all_scores.extend(generation_metrics)
    if faithfulness_scores:
        all_scores.extend(faithfulness_scores)
    if relevance_scores:
        all_scores.extend(relevance_scores)
    if overclaiming_scores:
        all_scores.extend(overclaiming_scores)

    if all_scores:
        aggregate["overall_average"] = round(sum(all_scores) / len(all_scores), 4)

    return aggregate
